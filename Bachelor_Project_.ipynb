{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bachelor_Project .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjM3zCgA0kxL"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFYXPf6j11XI",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "6d51138d-470a-4369-d42c-7b1e97f760af"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-91458da3-c95d-40a4-9291-a0c948484b4b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-91458da3-c95d-40a4-9291-a0c948484b4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhzvcZCF0lyI"
      },
      "source": [
        "class DeepVIV:\n",
        "    # Initialize the class\n",
        "    def __init__(self, t, x, y,\n",
        "                       u, v, eta,\n",
        "                       layers_uvp, layers_eta,\n",
        "                       Re):\n",
        "        \n",
        "        self.Re = Re\n",
        "        \n",
        "        X = np.concatenate([t, x, y], 1)\n",
        "        self.X_min = X.min(0)\n",
        "        self.X_max = X.max(0)\n",
        "        \n",
        "        # data on velocity (inside the domain)\n",
        "        self.t = t\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.u = u\n",
        "        self.v = v\n",
        "        self.eta = eta\n",
        "                \n",
        "        # layers\n",
        "        self.layers_uvp = layers_uvp\n",
        "        self.layers_eta  = layers_eta\n",
        "        \n",
        "        # initialize NN\n",
        "        self.weights_uvp, self.biases_uvp = self.initialize_NN(layers_uvp)\n",
        "        self.weights_eta, self.biases_eta = self.initialize_NN(layers_eta)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        # placeholders for data on velocities (inside the domain)\n",
        "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.y_tf = tf.placeholder(tf.float32, shape=[None, 1])        \n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.v_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.eta_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        \n",
        "        self.dummy_tf = tf.placeholder(tf.float32, shape=(None, layers_uvp[-1])) # dummy variable for fwd_gradients\n",
        "                \n",
        "        # physics informed neural networks (inside the domain)\n",
        "        (self.u_pred,\n",
        "         self.v_pred,\n",
        "         self.p_pred,\n",
        "         self.eta_pred,\n",
        "         self.eq1_pred,\n",
        "         self.eq2_pred,\n",
        "         self.eq3_pred) = self.net_VIV(self.t_tf, self.x_tf, self.y_tf)\n",
        "        \n",
        "        # loss\n",
        "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_tf - self.v_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.eta_tf - self.eta_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.eq1_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.eq2_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.eq3_pred))\n",
        "        \n",
        "        # optimizers\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
        "        self.train_op = self.optimizer.minimize(self.loss)\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = X\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.sin(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "    \n",
        "    def fwd_gradients(self, U, x):\n",
        "        g = tf.gradients(U, x, grad_ys=self.dummy_tf)[0]\n",
        "        return tf.gradients(g, self.dummy_tf)[0]\n",
        "            \n",
        "    def net_VIV(self, t, x, y):\n",
        "        X = 2.0*(tf.concat([t,x,y], 1) - self.X_min)/(self.X_max - self.X_min) - 1.0\n",
        "        uvp = self.neural_net(X, self.weights_uvp, self.biases_uvp)\n",
        "        \n",
        "        t_tmp = 2.0*(t - self.X_min[0])/(self.X_max[0] - self.X_min[0]) - 1\n",
        "        eta = self.neural_net(t_tmp, self.weights_eta, self.biases_eta)\n",
        "        \n",
        "        uvp_t = self.fwd_gradients(uvp, t)\n",
        "        uvp_x = self.fwd_gradients(uvp, x)\n",
        "        uvp_y = self.fwd_gradients(uvp, y)\n",
        "        uvp_xx = self.fwd_gradients(uvp_x, x)\n",
        "        uvp_yy = self.fwd_gradients(uvp_y, y)\n",
        "        \n",
        "        eta_t = tf.gradients(eta, t)[0]\n",
        "        eta_tt = tf.gradients(eta_t, t)[0]\n",
        "        \n",
        "        u = uvp[:,0:1]\n",
        "        v = uvp[:,1:2]\n",
        "        p = uvp[:,2:3]\n",
        "        \n",
        "        u_t = uvp_t[:,0:1]\n",
        "        v_t = uvp_t[:,1:2]\n",
        "        \n",
        "        u_x = uvp_x[:,0:1]\n",
        "        v_x = uvp_x[:,1:2]\n",
        "        p_x = uvp_x[:,2:3]\n",
        "        \n",
        "        u_y = uvp_y[:,0:1]\n",
        "        v_y = uvp_y[:,1:2]\n",
        "        p_y = uvp_y[:,2:3]\n",
        "        \n",
        "        u_xx = uvp_xx[:,0:1]\n",
        "        v_xx = uvp_xx[:,1:2]\n",
        "        \n",
        "        u_yy = uvp_yy[:,0:1]\n",
        "        v_yy = uvp_yy[:,1:2]\n",
        "        \n",
        "        eq1 = u_t + (u*u_x + v*u_y) + p_x - (1.0/self.Re)*(u_xx + u_yy) \n",
        "        eq2 = v_t + (u*v_x + v*v_y) + p_y - (1.0/self.Re)*(v_xx + v_yy) + eta_tt\n",
        "        eq3 = u_x + v_y\n",
        "        \n",
        "        return u, v, p, eta, eq1, eq2, eq3\n",
        "    \n",
        "    def train(self, num_epochs, batch_size, learning_rate):\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            N = self.t.shape[0]\n",
        "            perm = np.random.permutation(N)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            for it in range(0, N, batch_size):\n",
        "                idx = perm[np.arange(it,it+batch_size)]\n",
        "                (t_batch,\n",
        "                 x_batch,\n",
        "                 y_batch,\n",
        "                 u_batch,\n",
        "                 v_batch,\n",
        "                 eta_batch) = (self.t[idx,:],\n",
        "                               self.x[idx,:],\n",
        "                               self.y[idx,:],\n",
        "                               self.u[idx,:],\n",
        "                               self.v[idx,:],\n",
        "                               self.eta[idx,:])\n",
        "                \n",
        "                tf_dict = {self.t_tf: t_batch, self.x_tf: x_batch, self.y_tf: y_batch,\n",
        "                           self.u_tf: u_batch, self.v_tf: v_batch, self.eta_tf: eta_batch,\n",
        "                           self.dummy_tf: np.ones((batch_size, self.layers_uvp[-1])),\n",
        "                           self.learning_rate: learning_rate}\n",
        "                \n",
        "                self.sess.run(self.train_op, tf_dict)\n",
        "                \n",
        "                # Print\n",
        "                if it % (10*batch_size) == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    loss_value, learning_rate_value = self.sess.run([self.loss,self.learning_rate], tf_dict)\n",
        "                    print('Epoch: %d, It: %d, Loss: %.3e, Time: %.2f, Learning Rate: %.3e'\n",
        "                          %(epoch, it/batch_size, loss_value, elapsed, learning_rate_value))\n",
        "                    start_time = time.time()\n",
        "    \n",
        "    def predict(self, t_star, x_star, y_star):\n",
        "        \n",
        "        tf_dict = {self.t_tf: t_star, self.x_tf: x_star, self.y_tf: y_star}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
        "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
        "        eta_star = self.sess.run(self.eta_pred, tf_dict)\n",
        "        \n",
        "        return u_star, v_star, p_star, eta_star\n",
        "    \n",
        "    def predict_drag_lift(self, t_cyl):\n",
        "        \n",
        "        viscosity = (1.0/self.Re)\n",
        "        \n",
        "        theta = np.linspace(0.0,2*np.pi,200)[:,None] # N x 1\n",
        "        d_theta = theta[1,0] - theta[0,0]\n",
        "        x_cyl = 0.5*np.cos(theta) # N x 1\n",
        "        y_cyl = 0.5*np.sin(theta) # N x 1\n",
        "            \n",
        "        N = x_cyl.shape[0]\n",
        "        T = t_cyl.shape[0]\n",
        "        \n",
        "        T_star = np.tile(t_cyl, (1,N)).T # N x T\n",
        "        X_star = np.tile(x_cyl, (1,T)) # N x T\n",
        "        Y_star = np.tile(y_cyl, (1,T)) # N x T\n",
        "        \n",
        "        t_star = np.reshape(T_star,[-1,1]) # NT x 1\n",
        "        x_star = np.reshape(X_star,[-1,1]) # NT x 1\n",
        "        y_star = np.reshape(Y_star,[-1,1]) # NT x 1\n",
        "        \n",
        "        u_x_pred = tf.gradients(self.u_pred, self.x_tf)[0]\n",
        "        u_y_pred = tf.gradients(self.u_pred, self.y_tf)[0]\n",
        "        \n",
        "        v_x_pred = tf.gradients(self.v_pred, self.x_tf)[0]\n",
        "        v_y_pred = tf.gradients(self.v_pred, self.y_tf)[0]\n",
        "        \n",
        "        tf_dict = {self.t_tf: t_star, self.x_tf: x_star, self.y_tf: y_star}\n",
        "        \n",
        "        p_star, u_x_star, u_y_star, v_x_star, v_y_star = self.sess.run([self.p_pred, u_x_pred, u_y_pred, v_x_pred, v_y_pred], tf_dict)\n",
        "        \n",
        "        P_star = np.reshape(p_star, [N,T]) # N x T\n",
        "        P_star = P_star - np.mean(P_star, axis=0)\n",
        "        U_x_star = np.reshape(u_x_star, [N,T]) # N x T\n",
        "        U_y_star = np.reshape(u_y_star, [N,T]) # N x T\n",
        "        V_x_star = np.reshape(v_x_star, [N,T]) # N x T\n",
        "        V_y_star = np.reshape(v_y_star, [N,T]) # N x T\n",
        "    \n",
        "        INT0 = (-P_star[0:-1,:] + 2*viscosity*U_x_star[0:-1,:])*X_star[0:-1,:] + viscosity*(U_y_star[0:-1,:] + V_x_star[0:-1,:])*Y_star[0:-1,:]\n",
        "        INT1 = (-P_star[1: , :] + 2*viscosity*U_x_star[1: , :])*X_star[1: , :] + viscosity*(U_y_star[1: , :] + V_x_star[1: , :])*Y_star[1: , :]\n",
        "            \n",
        "        F_D = 0.5*np.sum(INT0.T+INT1.T, axis = 1)*d_theta # T x 1\n",
        "    \n",
        "        \n",
        "        INT0 = (-P_star[0:-1,:] + 2*viscosity*V_y_star[0:-1,:])*Y_star[0:-1,:] + viscosity*(U_y_star[0:-1,:] + V_x_star[0:-1,:])*X_star[0:-1,:]\n",
        "        INT1 = (-P_star[1: , :] + 2*viscosity*V_y_star[1: , :])*Y_star[1: , :] + viscosity*(U_y_star[1: , :] + V_x_star[1: , :])*X_star[1: , :]\n",
        "            \n",
        "        F_L = 0.5*np.sum(INT0.T+INT1.T, axis = 1)*d_theta # T x 1\n",
        "            \n",
        "        return F_D, F_L\n",
        "    \n",
        "def plot_solution(x_star, y_star, u_star, ax):\n",
        "    \n",
        "    nn = 200\n",
        "    x = np.linspace(x_star.min(), x_star.max(), nn)\n",
        "    y = np.linspace(y_star.min(), y_star.max(), nn)\n",
        "    X, Y = np.meshgrid(x,y)\n",
        "    \n",
        "    X_star = np.concatenate((x_star, y_star), axis=1)\n",
        "    \n",
        "    U_star = griddata(X_star, u_star.flatten(), (X, Y), method='linear')\n",
        "    \n",
        "    # h = ax.pcolor(X,Y,U_star, cmap = 'jet')\n",
        "    \n",
        "    h = ax.imshow(U_star, interpolation='nearest', cmap='jet', \n",
        "                  extent=[x_star.min(), x_star.max(), y_star.min(), y_star.max()],\n",
        "                  origin='lower', aspect='auto')\n",
        "    \n",
        "    return h\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P54JR9D1guj"
      },
      "source": [
        "data = scipy.io.loadmat('./Data/VIV_Concentration.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2SRLYSX1Mjy"
      },
      "source": [
        "if __name__ == \"__main__\": \n",
        "    \n",
        "    N_train  = 4000000\n",
        "        \n",
        "    layers_uvp = [3] + 10*[3*32] + [3]\n",
        "    layers_eta  = [1] + 10*[1*32] + [1]\n",
        "    \n",
        "    # Load Data\n",
        "    data = scipy.io.loadmat('./Data/VIV_Concentration.mat')\n",
        "    \n",
        "    t_star = data['t_star'] # T x 1\n",
        "    eta_star = data['eta_star'] # T x 1\n",
        "    \n",
        "    T = t_star.shape[0]\n",
        "        \n",
        "    X_star = data['X_star']\n",
        "    Y_star = data['Y_star']        \n",
        "    U_star = data['U_star']\n",
        "    V_star = data['V_star']\n",
        "    P_star = data['P_star']\n",
        "    \n",
        "    t = np.concatenate([t_star[i]+0.0*X_star[i,0] for i in range(0,T)])\n",
        "    x = np.concatenate([X_star[i,0] for i in range(0,T)])\n",
        "    y = np.concatenate([Y_star[i,0] for i in range(0,T)])\n",
        "    u = np.concatenate([U_star[i,0] for i in range(0,T)])\n",
        "    v = np.concatenate([V_star[i,0] for i in range(0,T)])\n",
        "    p = np.concatenate([P_star[i,0] for i in range(0,T)])\n",
        "    eta = np.concatenate([eta_star[i]+0.0*X_star[i,0] for i in range(0,T)])\n",
        "  \n",
        "    # Noiseles Data \n",
        "    # Training Data\n",
        "    idx = np.random.choice(t.shape[0], N_train, replace=False)\n",
        "    t_train = t[idx,:]\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "    p_train = p[idx,:]\n",
        "    eta_train = eta[idx,:]\n",
        "    \n",
        "    # Training\n",
        "    model = DeepVIV(t_train, x_train, y_train,\n",
        "                    u_train, v_train, eta_train,\n",
        "                    layers_uvp, layers_eta,\n",
        "                    Re = 100)\n",
        "    \n",
        "    model.train(num_epochs = 200, batch_size = 10000, learning_rate=1e-3)\n",
        "    model.train(num_epochs = 300, batch_size = 10000, learning_rate=1e-4)\n",
        "    model.train(num_epochs = 300, batch_size = 10000, learning_rate=1e-5)\n",
        "    model.train(num_epochs = 200, batch_size = 10000, learning_rate=1e-6)\n",
        "    \n",
        "    F_D, F_L = model.predict_drag_lift(t_star)\n",
        "    \n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax1.plot(t_star, F_D, 'b')\n",
        "    ax1.set_xlabel('$t$')\n",
        "    ax1.set_ylabel('$F_D$', color='b')\n",
        "    ax1.tick_params('y', colors='b')\n",
        "    \n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(t_star, F_L, 'r')\n",
        "    ax2.set_ylabel('$F_L$', color='r')\n",
        "    ax2.tick_params('y', colors='r')\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    \n",
        "    # savefig('./Figures/VIV_data_on_velocities_lift_drag', crop = False)\n",
        "    \n",
        "    # Test Data\n",
        "    snap = 100\n",
        "    t_test = t_star[snap] + 0.0*X_star[snap,0]\n",
        "    x_test = X_star[snap,0]\n",
        "    y_test = Y_star[snap,0]\n",
        "    \n",
        "    u_test = U_star[snap,0]\n",
        "    v_test = V_star[snap,0]\n",
        "    p_test = P_star[snap,0]\n",
        "    eta_test = eta_star[snap] + 0.0*X_star[snap,0]\n",
        "    \n",
        "    # Prediction\n",
        "    u_pred, v_pred, p_pred, eta_pred = model.predict(t_test, x_test, y_test)\n",
        "    \n",
        "    # Error\n",
        "    error_u = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
        "    error_v = np.linalg.norm(v_test-v_pred,2)/np.linalg.norm(v_test,2)\n",
        "    error_p = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
        "    error_eta = np.linalg.norm(eta_test-eta_pred,2)/np.linalg.norm(eta_test,2)\n",
        "\n",
        "    print('Error u: %e' % (error_u))\n",
        "    print('Error v: %e' % (error_v))\n",
        "    print('Error p: %e' % (error_p))\n",
        "    print('Error eta: %e' % (error_eta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB2kqfLk1Vlg"
      },
      "source": [
        "\n",
        "    #Plotting\n",
        "    circle11 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle12 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle21 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle22 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle31 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle32 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle41 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    circle42 = plt.Circle((0, 0), 0.5, facecolor='w', edgecolor='k')\n",
        "    \n",
        "    fig, ax = newfig(1.0, 1.6)\n",
        "    ax.axis('off')\n",
        "\n",
        "    gs = gridspec.GridSpec(4, 2)\n",
        "    gs.update(top=0.95, bottom=0.07, left=0.1, right=0.9, wspace=0.5, hspace=0.7)\n",
        "        \n",
        "    ########      Exact u(t,x,y)     ###########     \n",
        "    ax = plt.subplot(gs[0:1, 0])\n",
        "    h = plot_solution(x_test,y_test,u_test,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle21)\n",
        "    ax.axis('equal')\n",
        "    \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Exact $u(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    ########     Learned u(t,x,y)     ###########\n",
        "    ax = plt.subplot(gs[0:1, 1])\n",
        "    h = plot_solution(x_test,y_test,u_pred,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle22)\n",
        "    ax.axis('equal')\n",
        "    \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Learned $u(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    ########      Exact v(t,x,y)     ###########     \n",
        "    ax = plt.subplot(gs[1:2, 0])\n",
        "    h = plot_solution(x_test,y_test,v_test,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle31)\n",
        "    ax.axis('equal')\n",
        "    \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Exact $v(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    ########     Learned v(t,x,y)     ###########\n",
        "    ax = plt.subplot(gs[1:2, 1])\n",
        "    h = plot_solution(x_test,y_test,v_pred,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle32)\n",
        "    ax.axis('equal')\n",
        "    \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Learned $v(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    ########      Exact p(t,x,y)     ###########     \n",
        "    ax = plt.subplot(gs[2:3, 0])\n",
        "    h = plot_solution(x_test,y_test,p_test,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle41)\n",
        "    ax.axis('equal')    \n",
        "    \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Exact $p(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    ########     Learned p(t,x,y)     ###########\n",
        "    ax = plt.subplot(gs[2:3, 1])\n",
        "    h = plot_solution(x_test,y_test,p_pred,ax)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    ax.add_artist(circle42)\n",
        "    ax.axis('equal')\n",
        "        \n",
        "    fig.colorbar(h, cax=cax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('Learned $p(t,x,y)$', fontsize = 10)\n",
        "    \n",
        "    # savefig('./Figures/VIV_data_on_velocities', crop = False)\n",
        "    \n",
        "    #Save Data    \n",
        "    U_pred = np.zeros((T,), dtype=np.object)\n",
        "    V_pred = np.zeros((T,), dtype=np.object)\n",
        "    P_pred = np.zeros((T,), dtype=np.object)\n",
        "    C_pred = np.zeros((T,), dtype=np.object)\n",
        "    Eta_pred = np.zeros((T,), dtype=np.object)\n",
        "    for snap in range(0,t_star.shape[0]):\n",
        "        t_test = t_star[snap] + 0.0*X_star[snap,0]\n",
        "        x_test = X_star[snap,0]\n",
        "        y_test = Y_star[snap,0]\n",
        "        \n",
        "        u_test = U_star[snap,0]\n",
        "        v_test = V_star[snap,0]\n",
        "        p_test = P_star[snap,0]\n",
        "        eta_test = eta_star[snap] + 0.0*X_star[snap,0]\n",
        "    \n",
        "        # Prediction\n",
        "        u_pred, v_pred, p_pred, eta_pred = model.predict(t_test, x_test, y_test)\n",
        "        \n",
        "        U_pred[snap] = u_pred\n",
        "        V_pred[snap] = v_pred\n",
        "        P_pred[snap] = p_pred\n",
        "        Eta_pred[snap] = eta_pred\n",
        "    \n",
        "        # Error\n",
        "        error_u = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
        "        error_v = np.linalg.norm(v_test-v_pred,2)/np.linalg.norm(v_test,2)\n",
        "        error_p = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
        "        error_eta = np.linalg.norm(eta_test-eta_pred,2)/np.linalg.norm(eta_test,2)\n",
        "    \n",
        "        print('Error u: %e' % (error_u))\n",
        "        print('Error v: %e' % (error_v))\n",
        "        print('Error p: %e' % (error_p))\n",
        "        print('Error eta: %e' % (error_eta))\n",
        "    \n",
        "    scipy.io.savemat('./Results/VIV_data_on_velocities_results_%s.mat' %(time.strftime('%d_%m_%Y')),\n",
        "                     {'U_pred':U_pred, 'V_pred':V_pred, 'P_pred':P_pred, 'Eta_pred':Eta_pred, 'F_D':F_D, 'F_L':F_L})\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}